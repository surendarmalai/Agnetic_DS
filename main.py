from dotenv import load_dotenv
load_dotenv()
import pandas as pd
import io
from source_code.graph import ds_machine

def run_pipeline():
    file_path = "data\\telecom_churn_data.csv"  # Replace with your actual file path

    # ── Your SQL query ──────────────────────────────────────────────────────────
    with open("queries\\churn_query.sql", "r", encoding="utf-8") as f:
        sql_query = f.read()

    # ── Optional: Any rules you already know about this dataset ────────────────
    special_rules = ""

    # ── Extract metadata (OPTIMIZED FOR GROQ) ────────────────────────────────────
    print("1. Loading dataset and extracting metadata...")
    df = pd.read_csv(file_path)

    # 1. Get a simple dictionary of column names and their data types
    dtypes_dict = df.dtypes.astype(str).to_dict()
    
    # 2. Grab just ONE row as a compact dictionary to show the AI the data format
    sample_row = df.head(1).to_dict(orient='records')[0]

    # Combine them into a clean, minimal string
    dynamic_metadata = f"""
    DATASET PROFILE:

    1. COLUMNS & TYPES:
    {dtypes_dict}

    2. SAMPLE ROW:
    {sample_row}
    """

    print("   Metadata extracted. Passing to the AI...")

    # ── Build initial state ──────────────────────────────────────────────────────
    initial_input = {
        "file_path"       : file_path,
        "target_column"   : "churn",
        "metadata_summary": dynamic_metadata,
        "sql_query"       : sql_query,          # gives agent table context for alias resolution
        "df_columns"      : list(df.columns),   # raw column names for SQL alias stripping
        "special_rules"   : special_rules,       # your domain overrides
        "iteration_count" : 0
    }

    print("\n2. Starting the DS Machine...\n")

    for output in ds_machine.stream(initial_input):
        for node_name, state_updates in output.items():
            if "cleaning_code" in state_updates:
                print(f"--- CODE GENERATED BY {node_name.upper()} ---")
                print(state_updates['cleaning_code'])

            # Surfaces ambiguous fields inline during streaming
            if "ambiguous_fields" in state_updates:
                ambiguous = state_updates["ambiguous_fields"]
                if ambiguous:
                    print(f"\n⚠️  AMBIGUOUS FIELDS FLAGGED BY {node_name.upper()} — Review needed:")
                    for field in ambiguous:
                        print(f"  - {field.get('original_column')} → candidates: {field.get('candidates')}")
                        print(f"    reason: {field.get('reason')}\n")

if __name__ == "__main__":
    run_pipeline()