import pandas as pd
import io
from source_code.graph import ds_machine

def run_pipeline():
    file_path = "your_data.csv"  # Replace with your actual file path

    # ── Your SQL query ──────────────────────────────────────────────────────────
    # Place your query in query.sql in the same directory as this file.
    with open("query.sql", "r") as f:
        sql_query = f.read()

    # ── Optional: Any rules you already know about this dataset ────────────────
    # Leave as empty string if none. Examples:
    #   "data_vol fields are in KB"
    #   "aon is in days not months"
    #   "actv_dt is an integer date in YYYYMMDD format"
    special_rules = ""

    # ── Load data ───────────────────────────────────────────────────────────────
    print("1. Loading dataset and extracting rich metadata...")
    df = pd.read_csv(file_path)

    # ── Extract metadata ─────────────────────────────────────────────────────────
    head_str     = df.head().to_markdown()
    describe_str = df.describe(include='all').to_markdown()

    buffer = io.StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()

    dynamic_metadata = f"""
    DATASET PROFILE:

    1. DATAFRAME INFO (Types & Nulls):
    {info_str}

    2. STATISTICAL SUMMARY:
    {describe_str}

    3. FIRST 5 ROWS:
    {head_str}
    """

    print("   Metadata extracted. Passing to the AI...")

    # ── Build initial state ──────────────────────────────────────────────────────
    initial_input = {
        "file_path"       : file_path,
        "target_column"   : "churn",
        "metadata_summary": dynamic_metadata,
        "sql_query"       : sql_query,          # gives agent table context for alias resolution
        "df_columns"      : list(df.columns),   # raw column names for SQL alias stripping
        "special_rules"   : special_rules,       # your domain overrides
        "iteration_count" : 0
    }

    print("\n2. Starting the DS Machine...\n")

    for output in ds_machine.stream(initial_input):
        for node_name, state_updates in output.items():
            if "cleaning_code" in state_updates:
                print(f"--- CODE GENERATED BY {node_name.upper()} ---")
                print(state_updates['cleaning_code'])

            # Surfaces ambiguous fields inline during streaming
            if "ambiguous_fields" in state_updates:
                ambiguous = state_updates["ambiguous_fields"]
                if ambiguous:
                    print(f"\n⚠️  AMBIGUOUS FIELDS FLAGGED BY {node_name.upper()} — Review needed:")
                    for field in ambiguous:
                        print(f"  - {field.get('original_column')} → candidates: {field.get('candidates')}")
                        print(f"    reason: {field.get('reason')}\n")

if __name__ == "__main__":
    run_pipeline()